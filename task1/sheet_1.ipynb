{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5b6890",
   "metadata": {},
   "source": [
    "# Skalierbare Methoden der Künstlichen Intelligenz\n",
    "Charlotte Debus (charlotte.debus@kit.edu)\\\n",
    "Markus Götz (markus.goetz@kit.edu)\\\n",
    "Marie Weiel (marie.weiel@kit.edu)\n",
    "## Übung 2 am 22.11.22: Parallele k-Means-Clusteranalyse\n",
    "In der zweiten Übung beschäftigen wir uns mit der k-Means-Clusteranalyse und möglichen Parallelisierungsansätzen (siehe Vorlesung vom 10.11.22). Dazu verwenden wir den [Cityscapes](https://www.cityscapes-dataset.com/)-Datensatz. Dieser Datensatz enthält Stereovideosequenzen von Straßenszenen aus 50 verschiedenen Städten mit feinkörnigen Annotationen auf Pixelebene von 5000 hochaufgelösten Bildern.\n",
    "Jedes dieser Bilder besteht aus 2048 x 1024 Pixeln mit drei 256-Bit RGB-Farbkanälen pro Pixel, die in einer \"Short-Fat\"-Matrix mit 5000 x 6 291 456 seriellen Einträgen zusammengefasst sind:  \n",
    "5000 Bilder x (3 Kanäle x 2048 Pixel x 1024 Pixel) = 5000 x 6 291 456  \n",
    "Für unsere Aufgabe benutzen wir 300 dieser Samples. Sie finden diese auf dem bwUniCluster im Workspace `VL_ScalableAI` unter folgendem Pfad:  \n",
    "`/pfs/work7/workspace/scratch/ku4408-VL_ScalableAI/data/cityscapes_300.h5`\n",
    "### Aufgabe 1\n",
    "Untenstehend finden Sie eine serielle Implementierung des k-Means-Algorithmus in Python 3 unter Verwendung der Programmbibliothek für maschinelles Lernen [PyTorch](https://pytorch.org/). \n",
    "Führen Sie den Code auf einer CPU bzw. GPU auf dem bwUniCluster aus. Beachten Sie dabei, dass der Code für die GPU-Nutzung angepasst werden muss. Vergleichen Sie die Laufzeit. Was fällt Ihnen auf? \n",
    "\n",
    "*Hinweis: Laden Sie zunächst die benötigten Module auf dem bwUniCluster. Setzen Sie dann eine virtuelle Umgebung mit Python auf, in der Sie die benötigten Pythonpakete installieren. Erstellen Sie basierend auf untenstehendem Code ein Python-Skript, welches Sie mithilfe eines Bash-Skripts über SLURM auf dem Cluster submitten (siehe Übung vom 8.11.21). Nachfolgend finden Sie ein Template für das Submit-Skript für den CPU-Job inklusive der benötigten Module. Für die GPU-Nutzung müssen die #SBATCH-Optionen entsprechend angepasst werden. Weitere Informationen dazu finden Sie [hier](https://wiki.bwhpc.de/e/BwUniCluster_2.0_Slurm_common_Features).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=kmeans                  # job name\n",
    "#SBATCH --partition=single                 # queue for resource allocation\n",
    "#SBATCH --time=30:00                       # wall-clock time limit  \n",
    "#SBATCH --mem=30000                        # memory per node\n",
    "#SBATCH --nodes=1                          # number of nodes to be used\n",
    "#SBATCH --mail-type=ALL                    # Notify user by email when certain event types occur.\n",
    "#SBATCH --mail-user=u????@student.kit.edu  # notification email address\n",
    "\n",
    "module purge                                    # Unload all currently loaded modules.\n",
    "module load devel/cuda/10.2                     # Load required modules.  \n",
    "module load compiler/gnu/10.2\n",
    "module load mpi/openmpi/4.0  \n",
    "module load lib/hdf5/1.12.0-gnu-10.2-openmpi-4.0  \n",
    "source <path to your venv folder>/bin/activate  # Activate your virtual environment.\n",
    "\n",
    "python <path to your python script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e172b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters=8, init=\"random\", max_iter=300, tol=-1.0):\n",
    "        self.init = init             # initialization mode (default: random)\n",
    "        self.max_iter = max_iter     # maximum number of iterations\n",
    "        self.n_clusters = n_clusters # number of clusters\n",
    "        self.tol = tol               # tolerance for convergence criterion\n",
    "\n",
    "        self._inertia = float(\"nan\")\n",
    "        self._cluster_centers = None\n",
    "\n",
    "    def _initialize_centroids(self, x):\n",
    "        indices = torch.randperm(x.shape[0])[: self.n_clusters]\n",
    "        self._cluster_centers = x[indices]\n",
    "\n",
    "    def _fit_to_cluster(self, x):\n",
    "        distances = torch.cdist(x, self._cluster_centers)\n",
    "        matching_centroids = distances.argmin(axis=1, keepdim=True)\n",
    "\n",
    "        return matching_centroids\n",
    "\n",
    "    def fit(self, x):\n",
    "        self._initialize_centroids(x)\n",
    "        new_cluster_centers = self._cluster_centers.clone()\n",
    "\n",
    "        # Iteratively fit points to centroids.\n",
    "        for idx in range(self.max_iter):\n",
    "            # determine the centroids\n",
    "            print(\"Iteration\", idx, \"...\")\n",
    "            matching_centroids = self._fit_to_cluster(x)\n",
    "\n",
    "            # Update centroids.\n",
    "            for i in range(self.n_clusters):\n",
    "                # points in current cluster\n",
    "                selection = (matching_centroids == i).type(torch.int64)\n",
    "\n",
    "                # Accumulate points and total number of points in cluster.\n",
    "                assigned_points = (x * selection).sum(axis=0, keepdim=True)\n",
    "                points_in_cluster = selection.sum(axis=0, keepdim=True).clamp(\n",
    "                    1.0, torch.iinfo(torch.int64).max\n",
    "                )\n",
    "\n",
    "                # Compute new centroids.\n",
    "                new_cluster_centers[i : i + 1, :] = assigned_points / points_in_cluster\n",
    "\n",
    "            # Check whether centroid movement has converged.\n",
    "            self._inertia = ((self._cluster_centers - new_cluster_centers) ** 2).sum()\n",
    "            self._cluster_centers = new_cluster_centers.clone()\n",
    "            if self.tol is not None and self._inertia <= self.tol:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "print(\"PyTorch k-means clustering\")\n",
    "\n",
    "path = \"/pfs/work7/workspace/scratch/ku4408-VL_ScalableAI/data/cityscapes_300.h5\"\n",
    "dataset = \"cityscapes_data\"\n",
    "\n",
    "print(\"Loading data... {}[{}]\".format(path, dataset), end=\"\")\n",
    "print(\"\\n\")\n",
    "# Data is available in HDF5 format.\n",
    "# An HDF5 file is a container for two kinds of objects:\n",
    "# - datasets: array-like collections of data\n",
    "# - groups: folder-like containers holding datasets and other groups\n",
    "# Most fundamental thing to remember when using h5py is:\n",
    "# Groups work like dictionaries, and datasets work like NumPy arrays.\n",
    "\n",
    "# Open file for reading. We use the Cityscapes dataset.\n",
    "\n",
    "with h5py.File(path, \"r\") as handle:\n",
    "    print(\"Open h5 file...\")\n",
    "    data = torch.tensor(handle[dataset][:300], device=\"cpu\") # default: device =\"cpu\"; set device=\"cuda\" for GPU\n",
    "print(\"Torch tensor created...\")\n",
    "\n",
    "# k-means parameters\n",
    "num_clusters = 8\n",
    "num_iterations = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=num_iterations)\n",
    "print(\"Start fitting the data...\")\n",
    "start = time.perf_counter() # Start runtime measurement.\n",
    "kmeans.fit(data)            # Perform actual k-means clustering.\n",
    "end = time.perf_counter()   # Stop runtime measurement.\n",
    "print(\"DONE.\")\n",
    "print(\"Run time:\",\"\\t{}s\".format(end - start), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7900f7",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "Implementieren Sie ausgehend von obigem Code eine Sample-parallele Version des k-Means-Algorithmus. Orientieren Sie sich dabei an der obenstehenden seriellen Implementierung. \n",
    "Das Interface bzw. die Benutzung der Klasse im eigentlichen Ausführungsteil des Codes soll gleich bleiben. Für die Parallelisierung benötigen Sie einen entsprechend parallelisierten Dataloader. Diesen finden Sie im untenstehenden Code-Fragment. Testen Sie Ihren Code auf vier Knoten des bwUniClusters. Untenstehend finden Sie ein entsprechendes Submit-Skript in bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691eeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=kmeans                  # job name\n",
    "#SBATCH --partition=multiple               # queue for the resource allocation.\n",
    "#SBATCH --time=30:00                       # wall-clock time limit  \n",
    "#SBATCH --mem=30000                        # memory per node\n",
    "#SBATCH --nodes=4                          # number of nodes to be used\n",
    "#SBATCH --cpus-per-task=40                 # number of CPUs required per MPI task\n",
    "#SBATCH --ntasks-per-node=1                # maximum count of tasks per node\n",
    "#SBATCH --mail-type=ALL                    # Notify user by email when certain event types occur.\n",
    "#SBATCH --mail-user=u????@student.kit.edu  # notification email address\n",
    "\n",
    "export OMP_NUM_THREADS=40\n",
    "\n",
    "module purge                                    # Unload all currently loaded modules.\n",
    "module load devel/cuda/10.2                     # Load required modules.  \n",
    "module load compiler/gnu/10.2\n",
    "module load mpi/openmpi/4.0  \n",
    "module load lib/hdf5/1.12.0-gnu-10.2-openmpi-4.0\n",
    "source <path to your venv folder>/bin/activate  # Activate your virtual environment.\n",
    "\n",
    "mpirun python <path to your python script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "from mpi4py import MPI\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "# Implementierung Sample-parallele Version HIER.\n",
    "\n",
    "rank = MPI.COMM_WORLD.rank\n",
    "size = MPI.COMM_WORLD.size\n",
    "\n",
    "if rank==0: print(\"PyTorch k-means clustering\")\n",
    "\n",
    "path = \"/pfs/work7/workspace/scratch/ku4408-VL_ScalableAI/data/cityscapes_300.h5\"\n",
    "dataset = \"cityscapes_data\"\n",
    "\n",
    "if rank==0: \n",
    "    print(\"Loading data... {}[{}]\".format(path, dataset), end=\"\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "with h5py.File(path, \"r\") as handle:\n",
    "    chunk = int(handle[dataset].shape[0]/size)\n",
    "    if rank==size-1: data = torch.tensor(handle[dataset][rank*chunk:])\n",
    "    else: data = torch.tensor(handle[dataset][rank*chunk:(rank+1)*chunk])\n",
    "\n",
    "print(\"\\t[OK]\")\n",
    "\n",
    "# k-means parameters\n",
    "num_clusters = 8\n",
    "num_iterations = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=num_iterations)\n",
    "if rank==0: \n",
    "    print(\"Start fitting the data...\")\n",
    "    start = time.perf_counter() # Start runtime measurement.\n",
    "    \n",
    "kmeans.fit(data)            # Perform actual k-means clustering.\n",
    "\n",
    "if rank==0: \n",
    "    end = time.perf_counter()   # Stop runtime measurement.\n",
    "    print(\"DONE.\")\n",
    "    print(\"Run time:\",\"\\t{}s\".format(end - start), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5379ea7",
   "metadata": {},
   "source": [
    "### Aufgabe 3\n",
    "Implementieren Sie ausgehend von obigem Code eine Feature-parallele Version des k-Means-Algorithmus. Den entsprechend parallelisierten Dataloader finden Sie im untenstehenden Code-Fragment. Testen Sie Ihren Code auf vier Knoten des bwUniClusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217173aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "from mpi4py import MPI\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "# Implementierung Feature-parallele Version HIER.\n",
    "\n",
    "rank = MPI.COMM_WORLD.rank\n",
    "size = MPI.COMM_WORLD.size\n",
    "\n",
    "if rank==0: print(\"PyTorch k-means clustering\")\n",
    "\n",
    "path = \"/pfs/work7/workspace/scratch/ku4408-VL_ScalableAI/data/cityscapes_300.h5\"\n",
    "dataset = \"cityscapes_data\"\n",
    "\n",
    "if rank==0: print(\"Loading data... {}[{}]\".format(path, dataset), end=\"\")\n",
    "\n",
    "with h5py.File(path, \"r\") as handle:\n",
    "    chunk = int(handle[dataset].shape[1]/size)\n",
    "    if rank==size-1: data = torch.tensor(handle[dataset][:,rank*chunk:])\n",
    "    else: data = torch.tensor(handle[dataset][:,rank*chunk:(rank+1)*chunk])\n",
    "\n",
    "print(\"\\t[OK]\")\n",
    "\n",
    "# k-means parameters\n",
    "num_clusters = 8\n",
    "num_iterations = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=num_iterations)\n",
    "\n",
    "if rank==0: \n",
    "    print(\"Start fitting the data...\")\n",
    "    start = time.perf_counter() # Start runtime measurement.\n",
    "\n",
    "kmeans.fit(data)            # Perform actual k-means clustering.\n",
    "\n",
    "if rank==0:\n",
    "    end = time.perf_counter()   # Stop runtime measurement.\n",
    "    print(\"DONE.\")\n",
    "    print(\"Run time:\",\"\\t{}s\".format(end - start), \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
